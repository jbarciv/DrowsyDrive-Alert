{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "863abe1d-433a-4fcc-ac81-89224608f569",
   "metadata": {},
   "source": [
    "# DrowsyDrive-Alert\n",
    "\n",
    "## **Driving Drowsiness Detection**\n",
    "\n",
    "*Master's in Automation and Robotics - ETSII (UPM)* \\\n",
    "**Subject:** Computer Vision\\\n",
    "**Course:** 2023-24\\\n",
    "**Student Name (ID):**\n",
    "- Ivonne Quishpe (23146)\n",
    "- Gustavo Maldonado (23102)\n",
    "- Jorge Guijarro (23075)\n",
    "- Micaela Cabrera (23023)\n",
    "- Josep Mª Barberá (17048)\n",
    "\n",
    "**Date:** December, 6\n",
    "\n",
    "This Notebook implements a drowsiness detection system using MediaPipe. From the live camera or already recorded videos, the most relevant facial features regarding eyes and mouth are extracted from each frame. Once these coordinates are obtained, the Eye Aspect Ratio (EAR) and Mouth Aspect Ratio (MAR) are calculated to determine whether the eyes or mouth are open or closed. Thus, once the capture speed (frames per second) is known, it is possible to determine whether the person is drowsy or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bef714c-7699-4d0b-97b7-a4fa78e3847c",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14830ba4-9e04-49bd-81e3-9d8420786d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "from mediapipe import solutions\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e401fcd-e371-42be-b172-401f452684db",
   "metadata": {},
   "source": [
    "#### Parameters\n",
    "\n",
    "We define the face landmarks to be used in the EAR and MAR calculation. All the landmarks numbers are shown [here](https://storage.googleapis.com/mediapipe-assets/documentation/mediapipe_face_landmark_fullsize.png).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d43c8ca-ab16-4247-b959-23ca15330465",
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_landmarks_num = 6   # 4 or 6\n",
    "\n",
    "if eye_landmarks_num == 4:\n",
    "    left_landmarks = [362, 386, 263, 374]\n",
    "    right_landmarks = [33, 158, 133, 153]\n",
    "else:\n",
    "    left_landmarks = [362, 384, 386, 263, 374, 381]\n",
    "    right_landmarks = [33, 160, 158, 133, 153, 144]\n",
    "    if eye_landmarks_num != 6:\n",
    "        print(\"Number of landmarks per eye should be 4 or 6. Setting it to 6 by default...\")\n",
    "    \n",
    "mouth_landmarks = [78, 81, 13, 311, 308, 402, 14, 178]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2344d610-6fdc-4241-898a-f246a24dbd23",
   "metadata": {},
   "source": [
    "#### Save the landmarks coordinates for eyes and mouth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39be4749-1b72-4944-88c2-716829295510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_face_points(face_landmarks):\n",
    "    # Save the landmark coordinates for  the eyes\n",
    "    right_eye_points = []\n",
    "    for landmark in right_landmarks:\n",
    "        right_eye_points.append([face_landmarks[landmark].x,\n",
    "                            face_landmarks[landmark].y,\n",
    "                            face_landmarks[landmark].z])\n",
    "\n",
    "    left_eye_points = []\n",
    "    for landmark in left_landmarks:\n",
    "        left_eye_points.append([face_landmarks[landmark].x,\n",
    "                            face_landmarks[landmark].y,\n",
    "                            face_landmarks[landmark].z])\n",
    "\n",
    "    # Save the landmark coordinates for  the mouth\n",
    "    mouth_points = []\n",
    "    for landmark in mouth_landmarks:\n",
    "        mouth_points.append([face_landmarks[landmark].x,\n",
    "                            face_landmarks[landmark].y,\n",
    "                            face_landmarks[landmark].z])\n",
    "    return right_eye_points, left_eye_points, mouth_points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdbf808-2348-4539-86cf-a12f74ac2159",
   "metadata": {},
   "source": [
    "#### Compute the Eye Aspect Ratio (EAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ab917b0-eb29-4f93-b7f1-726dd44ec586",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_EARs(l_p, r_p):\n",
    "    if eye_landmarks_num == 4:\n",
    "        p2_p4 = np.linalg.norm(l_p[1]-l_p[3])\n",
    "        p1_p3 = np.linalg.norm(l_p[0]-l_p[2])\n",
    "        left_EAR = (p2_p4)/(p1_p3)\n",
    "\n",
    "        p2_p4 = np.linalg.norm(r_p[1]-r_p[3])\n",
    "        p1_p3 = np.linalg.norm(r_p[0]-r_p[2])\n",
    "        right_EAR = (p2_p4)/(p1_p3)\n",
    "    else:\n",
    "        p2_p6 = np.linalg.norm(l_p[1]-l_p[5])\n",
    "        p3_p5 = np.linalg.norm(l_p[2]-l_p[4])\n",
    "        p1_p4 = np.linalg.norm(l_p[0]-l_p[3])\n",
    "        left_EAR = (p2_p6 + p3_p5)/(2*p1_p4)\n",
    "\n",
    "        p2_p6 = np.linalg.norm(r_p[1]-r_p[5])\n",
    "        p3_p5 = np.linalg.norm(r_p[2]-r_p[4])\n",
    "        p1_p4 = np.linalg.norm(r_p[0]-r_p[3])\n",
    "        right_EAR = (p2_p6 + p3_p5)/(2*p1_p4)\n",
    "\n",
    "    return left_EAR, right_EAR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab7d23e-8787-4ea7-a830-0bd0ed6d5a8d",
   "metadata": {},
   "source": [
    "#### Compute the Mouth Aspect Ratio (MAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f5ca825-be22-4d71-8bea-5d5d3deb059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MAR(m_p):\n",
    "  p2_p8 = np.linalg.norm(m_p[1]-m_p[7])\n",
    "  p3_p7= np.linalg.norm(m_p[2]-m_p[6])\n",
    "  p4_p6 = np.linalg.norm(m_p[3]-m_p[5])\n",
    "  p1_p5 = np.linalg.norm(m_p[0]-m_p[4])\n",
    "  MAR = (p2_p8 + p3_p7 + p4_p6)/(2*p1_p5)\n",
    "  return MAR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab84299a",
   "metadata": {},
   "source": [
    "#### Actualize the state of the person due to the past and current EAR and MAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11b2e910-b0d1-45d7-9c80-406008d189b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_drowsiness(left_EAR, right_EAR, MAR, data):\n",
    "    left_score, right_score, mouth_score, eyes_tag, yawn_tag, alert, frames_sleep, frames_awake = data\n",
    "    \n",
    "    ## Increase score if eyes closed, decrease otherwise\n",
    "    if (left_EAR < 0.15):\n",
    "        left_score += 1\n",
    "    else:\n",
    "        left_score -= 1\n",
    "        \n",
    "    if (right_EAR < 0.15):\n",
    "        right_score += 1\n",
    "    else: \n",
    "        right_score -= 1\n",
    "\n",
    "    ## Increase score if mouth open, decrease otherwise\n",
    "    if (MAR > 0.3):\n",
    "        mouth_score += 1\n",
    "    else: \n",
    "        mouth_score -= 1\n",
    "\n",
    "    ## Scores can't be negative\n",
    "    if (left_score < 0):\n",
    "        left_score = 0\n",
    "    if (right_score < 0):\n",
    "        right_score = 0\n",
    "    if (mouth_score < 0):\n",
    "        mouth_score = 0\n",
    "\n",
    "    ## Un parpadeo normal equivale a 9 frames (300 ms si se toman 30 fps).\n",
    "    if (left_score > 12 and right_score > 12):\n",
    "        eyes_tag = \"closed\"\n",
    "    else:\n",
    "        eyes_tag = \"open\"\n",
    "        \n",
    "    ## Si la boca está abierta más de tres segundos se considera bostezo\n",
    "    if (mouth_score > 70):\n",
    "        yawn_tag = True\n",
    "    else:\n",
    "        yawn_tag = False\n",
    "\n",
    "    if (eyes_tag == \"closed\" or yawn_tag == True):\n",
    "        alert += 1\n",
    "        frames_sleep += 1\n",
    "    else:\n",
    "        alert -= 10\n",
    "        if (alert < 0):\n",
    "            alert = 0\n",
    "        frames_awake += 1\n",
    "    return [left_score, right_score, mouth_score, eyes_tag, yawn_tag, alert, frames_sleep, frames_awake]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6148893-75c2-43f4-9bf1-a89e21b4b207",
   "metadata": {},
   "source": [
    "#### Live Video Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bafe2ba-a6a8-4101-bffe-220c6f23108a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1704282615.071553   11475 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1704282615.075422   11617 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 Mesa 21.2.6), renderer: Mesa Intel(R) UHD Graphics 620 (KBL GT2)\n",
      "W0000 00:00:1704282615.076589   11475 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n"
     ]
    }
   ],
   "source": [
    "## Define your captura number device here:\n",
    "\n",
    "# camera = 2 # this is for selecting my IR Camera\n",
    "camera = 0 # this is for selecting my normal Camera\n",
    "\n",
    "cap = cv2.VideoCapture(camera)\n",
    "# frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "# print(frame_rate)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH) + 0.5)\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT) + 0.5)\n",
    "size = (width, height)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "out = cv2.VideoWriter('your_video.avi', fourcc, 20.0, size)\n",
    "index_list = mouth_landmarks + left_landmarks + right_landmarks\n",
    "\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "FaceLandmarker = mp.tasks.vision.FaceLandmarker\n",
    "FaceLandmarkerOptions = mp.tasks.vision.FaceLandmarkerOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "options = FaceLandmarkerOptions(\n",
    "    base_options=BaseOptions(model_asset_path='face_landmarker_v2_with_blendshapes.task'),\n",
    "    running_mode=VisionRunningMode.IMAGE)\n",
    "\n",
    "i = 0\n",
    "prev_left_EAR, prev_right_EAR, prev_MAR = 0,0,0\n",
    "data = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "alert_time = 0\n",
    "with FaceLandmarker.create_from_options(options) as landmarker:\n",
    "    while True:\n",
    "        i += 1\n",
    "        ret, frame = cap.read()\n",
    "        if ret == False:\n",
    "            break\n",
    "        height, width, _ = frame.shape\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)\n",
    "        results = landmarker.detect(mp_image)\n",
    "\n",
    "        if results.face_landmarks is not None:\n",
    "            face_landmarks_list = results.face_landmarks\n",
    "            for idx in range(len(face_landmarks_list)):\n",
    "                face_landmarks = face_landmarks_list[idx]\n",
    "                face_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "                face_landmarks_proto.landmark.extend([\n",
    "                    landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in face_landmarks])\n",
    "                for index in index_list:\n",
    "                    x = int(face_landmarks[index].x * width)\n",
    "                    y = int(face_landmarks[index].y * height)\n",
    "                    cv2.circle(frame, (x, y), 2, (0, 255, 0), 1)\n",
    "\n",
    "        try:\n",
    "            right_eye_points, left_eye_points, mouth_points = get_face_points(results.face_landmarks[0])\n",
    "        except:\n",
    "            right_eye_points = right_eye_points\n",
    "            left_eye_points = left_eye_points \n",
    "            mouth_points = mouth_points\n",
    "            \n",
    "        left_EAR, right_EAR = get_EARs(np.array(left_eye_points), np.array(right_eye_points))\n",
    "        MAR = get_MAR(np.array(mouth_points))\n",
    "\n",
    "        data = check_drowsiness(left_EAR, right_EAR, MAR, data)\n",
    "        _, _, _, eyes_tag, yawn_tag, alert, frames_sleep, frames_awake = data\n",
    "        \n",
    "        if i > 7 and i % 7 == 0:\n",
    "            if (eyes_tag == \"open\"):\n",
    "                eyes_color = (0, 255, 255)\n",
    "            else:\n",
    "                eyes_color = (0, 0, 255)\n",
    "            if (yawn_tag == True):\n",
    "                mouth_color = (0, 0, 255)\n",
    "            else:\n",
    "                mouth_color = (0, 255, 255)\n",
    "            cv2.putText(frame,f\"Left EAR: {left_EAR:.2f}\",(30, 30),cv2.FONT_HERSHEY_DUPLEX,0.7,eyes_color,1,)\n",
    "            cv2.putText(frame,f\"Right EAR: {right_EAR:.2f}\",(30, 60),cv2.FONT_HERSHEY_DUPLEX,0.7,eyes_color,1,)\n",
    "            cv2.putText(frame,f\"MAR: {MAR:.2f}\",(30, 90),cv2.FONT_HERSHEY_DUPLEX,0.7,mouth_color,1,)\n",
    "            prev_left_EAR, prev_right_EAR, prev_MAR = left_EAR, right_EAR, MAR\n",
    "        else:\n",
    "            if (eyes_tag == \"open\"):\n",
    "                eyes_color = (0, 255, 255)\n",
    "            else:\n",
    "                eyes_color = (0, 0, 255)\n",
    "            if (yawn_tag == True):\n",
    "                mouth_color = (0, 0, 255)\n",
    "            else:\n",
    "                mouth_color = (0, 255, 255)\n",
    "            cv2.putText(frame,f\"Left EAR: {prev_left_EAR:.2f}\",(30, 30),cv2.FONT_HERSHEY_DUPLEX,0.7,eyes_color,1,)\n",
    "            cv2.putText(frame,f\"Right EAR: {prev_right_EAR:.2f}\",(30, 60),cv2.FONT_HERSHEY_DUPLEX,0.7,eyes_color,1,)\n",
    "            cv2.putText(frame,f\"MAR: {prev_MAR:.2f}\",(30, 90),cv2.FONT_HERSHEY_DUPLEX,0.7,mouth_color,1,)\n",
    "\n",
    "        if (alert > 0):\n",
    "            alert_time += 1\n",
    "            cv2.rectangle(frame, (0,0), (frame.shape[1]-1, frame.shape[0]-1), (0,0,255), 20)\n",
    "        cv2.imshow(\"Frame\", frame)\n",
    "        out.write(frame)\n",
    "        k = cv2.waitKey(1) & 0xFF\n",
    "        if k == 27:\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e579196b-4714-4006-a7ef-f7eb6bd17fce",
   "metadata": {},
   "source": [
    "#### Video processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "480fc033-0bf3-4ccf-b412-a32dd09b3145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_mesh(path):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "\n",
    "    index_list = mouth_landmarks + left_landmarks + right_landmarks\n",
    "\n",
    "    BaseOptions = mp.tasks.BaseOptions\n",
    "    FaceLandmarker = mp.tasks.vision.FaceLandmarker\n",
    "    FaceLandmarkerOptions = mp.tasks.vision.FaceLandmarkerOptions\n",
    "    VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "    options = FaceLandmarkerOptions(\n",
    "        base_options=BaseOptions(model_asset_path='face_landmarker_v2_with_blendshapes.task'),\n",
    "        running_mode=VisionRunningMode.IMAGE)\n",
    "\n",
    "    i = 0\n",
    "    prev_left_EAR, prev_right_EAR, prev_MAR = 0,0,0\n",
    "    data = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    alert_time = 0\n",
    "    right_eye_points = [[0.5324623584747314, 0.5747805833816528, 0.004818260669708252], \n",
    "                        [0.5445109009742737, 0.5637452006340027, -0.0011025648564100266], \n",
    "                        [0.5702856183052063, 0.5580060482025146, -0.0021580627653747797], \n",
    "                        [0.5986989736557007, 0.5627603530883789, 0.023030918091535568], \n",
    "                        [0.5674318075180054, 0.5806047916412354, 0.004358564969152212], \n",
    "                        [0.5431403517723083, 0.5771798491477966, 0.00291746249422431]]\n",
    "    \n",
    "    left_eye_points =  [[0.37691813707351685, 0.5854962468147278, 0.002876535290852189], \n",
    "                        [0.3939114511013031, 0.5759596824645996, -0.014068366028368473], \n",
    "                        [0.4192018210887909, 0.5710769891738892, -0.016808023676276207], \n",
    "                        [0.4473620355129242, 0.5828512907028198, -0.004112580791115761], \n",
    "                        [0.42484530806541443, 0.592927098274231, -0.010263157077133656], \n",
    "                        [0.3993476927280426, 0.597295880317688, -0.0077733625657856464]]\n",
    "    mouth_points = [[0.46936848759651184, 0.8040176630020142, 0.005104257259517908], \n",
    "                    [0.49690064787864685, 0.7995496392250061, -0.012885571457445621], \n",
    "                    [0.5246973037719727, 0.7988075017929077, -0.0167536623775959], \n",
    "                    [0.5486900210380554, 0.7920270562171936, -0.006666502915322781], \n",
    "                    [0.572444498538971, 0.7885196208953857, 0.017038118094205856], \n",
    "                    [0.5485737919807434, 0.7921735644340515, -0.0028128675185143948], \n",
    "                    [0.5245987176895142, 0.7992886304855347, -0.012132205069065094], \n",
    "                    [0.4968000650405884, 0.8000173568725586, -0.009343835525214672]]\n",
    "\n",
    "    with FaceLandmarker.create_from_options(options) as landmarker:\n",
    "        while True:\n",
    "            i += 1\n",
    "            ret, frame = cap.read()\n",
    "            if ret == False:\n",
    "                break\n",
    "            height, width, _ = frame.shape\n",
    "            mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)\n",
    "            results = landmarker.detect(mp_image)\n",
    "\n",
    "            if results.face_landmarks is not None:\n",
    "                face_landmarks_list = results.face_landmarks\n",
    "                for idx in range(len(face_landmarks_list)):\n",
    "                    face_landmarks = face_landmarks_list[idx]\n",
    "                    face_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "                    face_landmarks_proto.landmark.extend([\n",
    "                        landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in face_landmarks])\n",
    "                    for index in index_list:\n",
    "                        x = int(face_landmarks[index].x * width)\n",
    "                        y = int(face_landmarks[index].y * height)\n",
    "                        cv2.circle(frame, (x, y), 2, (0, 255, 0), 1)\n",
    "\n",
    "            try:\n",
    "                right_eye_points, left_eye_points, mouth_points = get_face_points(results.face_landmarks[0])\n",
    "            except:\n",
    "                right_eye_points = right_eye_points\n",
    "                left_eye_points = left_eye_points\n",
    "                mouth_points = mouth_points\n",
    "\n",
    "            left_EAR, right_EAR = get_EARs(np.array(left_eye_points), np.array(right_eye_points))\n",
    "            MAR = get_MAR(np.array(mouth_points))\n",
    "            # print(left_EAR, right_EAR, MAR)\n",
    "\n",
    "            data = check_drowsiness(left_EAR, right_EAR, MAR, data)\n",
    "            _, _, _, eyes_tag, yawn_tag, alert, frames_sleep, frames_awake = data\n",
    "\n",
    "            if (alert > 0):\n",
    "                alert_time += 1\n",
    "            # cv2_imshow(frame)\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    return alert_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da004b79",
   "metadata": {},
   "source": [
    "#### Compute the Drowsyness for a list of videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15111a55-d77e-44d1-86f8-4651657d54ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "files = glob.glob(\"/home/josep/Desktop/*.mp4\")\n",
    "for my_file in files:\n",
    "    result = face_mesh(my_file)\n",
    "    results.append(result)\n",
    "    print(f\"Processing file: {my_file} with results= {result}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
